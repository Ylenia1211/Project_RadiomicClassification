{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codeCNR_siPca.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbLjAhVJOxl"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import  MinMaxScaler #stardadizzo i valori a intervallo [0,1]\n",
        "from sklearn.preprocessing import  StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KPpx3rYJ_x2"
      },
      "source": [
        "input = pd.read_csv(\"/content/drive/My Drive/dati/dati_experiment/file2.csv\") \n",
        "data_obj = pd.DataFrame(input)\n",
        "data_obj_droped = data_obj.drop(columns=['PSA', 'ETA','STATUS','Tipologia'])\n",
        "\n",
        "col_status = data_obj['STATUS']\n",
        "col_typology = data_obj['Tipologia']\n",
        "\n",
        "# create a dictonary \n",
        "data = {\"STATUS\": col_status, \"Tipologia\": col_typology} \n",
        "data_class = pd.concat(data, axis = 1) \n",
        "\n",
        "# show the dataframe \n",
        "tuple_class = list(map(tuple, data_class.to_numpy())) #m=2, t=1, n=0\n",
        "#print(len(tuple_class))  #45 sample\n",
        "encode_tuple = { (0,0) : 1, (0,1) : 2, (0,2) : 3, (1,0) : 4, (1,1) : 5, (1,2) : 6}\n",
        "macro_class = []\n",
        "for couple in tuple_class: \n",
        "  #print(encode_tuple[couple])\n",
        "  macro_class.append(encode_tuple[couple])\n",
        "\n",
        "#print(macro_class)\n",
        "series_macro_class = pd.Series((v for v in macro_class) )\n",
        "#print(series_macro_class)\n",
        "print(\"NUMERO DI TUPLE PER CLASSE MACRO:\")\n",
        "series_macro_class.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbh6eeasKBaZ"
      },
      "source": [
        "n_fold = 5\n",
        "fold_obj = StratifiedKFold(n_fold ,  True, random_state = 4)  # 5 fold, shuffle=true\n",
        "print(fold_obj)\n",
        "print(len(data_obj_droped)) #45\n",
        "y = data_obj['STATUS']\n",
        "\n",
        "#Strutture utili per immagazzinamento risultati degli esperimenti\n",
        "#n = data_objs_oversampling #.to_numpy()\n",
        "n = data_obj_droped\n",
        "test_predicted_logit = np.zeros((np.array(n).shape[0],))\n",
        "test_predicted_svc = np.zeros((np.array(n).shape[0],))\n",
        "test_predicted_bayes = np.zeros((np.array(n).shape[0],))\n",
        "test_predicted_clf =  np.zeros((np.array(n).shape[0],))\n",
        "test_predicted_knn =  np.zeros((np.array(n).shape[0],))\n",
        "\n",
        "#STANDARDIZZAZIONE\n",
        "#StandardScaler(with_std=True) o #MinMaxScaler()\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_obj_droped) # data_obj_droped: dataframe senza le colonne: PSA ETA  STATUS Tipologia\n",
        "data_objs = scaler.transform(data_obj_droped) \n",
        "data_objs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2-kCj4gLlQl"
      },
      "source": [
        "#funzione oversampling \"normale\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2BFLsUULpS_"
      },
      "source": [
        "'''\n",
        "#Funzione per fare oversampling solo sul training (nella fase di K-fold split)\n",
        "def oversampling_training(dataset_training, y_status, y_tipology): \n",
        "  data = {\"STATUS\": y_status, \"Tipologia\": y_tipology} \n",
        "  data_class = pd.concat(data, axis = 1) \n",
        "  tuple_class = list(map(tuple, data_class.to_numpy()))\n",
        "  #print(len(tuple_class))  #45 sample\n",
        "  encode_tuple = { (0,0) : 1, (0,1) : 2, (0,2) : 3, (1,0) : 4, (1,1) : 5, (1,2) : 6}\n",
        "  macro_class = []\n",
        "  for couple in tuple_class: \n",
        "    macro_class.append(encode_tuple[couple])\n",
        "\n",
        "  series_macro_class = pd.Series((v for v in macro_class) )\n",
        "  #print(series_macro_class)\n",
        "  #print(\"NUMERO DI TUPLE PER CLASSE MACRO:\")\n",
        "  #print(series_macro_class.value_counts())\n",
        "\n",
        "  #oversampling \"NORMALE\"                                \n",
        "  oversample = SMOTE(k_neighbors=1)#Expected n_neighbors <= n_samples,  but n_samples = 2, n_neighbors = 3\n",
        "  data_objs_oversampling, macro_class_oversampling = oversample.fit_resample(dataset_training, series_macro_class)\n",
        "\n",
        "  macro= pd.Series((v for v in macro_class_oversampling) )\n",
        "  #print(series_macro_class) \n",
        "  print(\"NUMERO DI TUPLE PER CLASSE MACRO:\")\n",
        "  print(macro.value_counts())\n",
        "  #DALL'OVERSAMPLING DELLA MACRO_CLASS DEVO CODIFICARE LE NUOVE ETICHETTE\n",
        "  micro_class = list()\n",
        "  for encoded in macro_class_oversampling: \n",
        "    #print(encoded)\n",
        "    for key, value in encode_tuple.items():  #  in dictionary.iteritems():  (for Python 2.x)\n",
        "      if encoded == value:\n",
        "          #print(key)\n",
        "          micro_class.append(key)\n",
        "  #print(micro_class)\n",
        "  new_df = pd.DataFrame(micro_class, columns=['STATUS', 'Tipologia'])\n",
        "  #print(new_df)\n",
        "  y=new_df['STATUS']\n",
        "  #print(y)\n",
        "  #print(\"Nuova lunghezza del vettore target y (dopo l'oversampling): \", len(y))\n",
        "  return data_objs_oversampling, y\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtqyilOpLdKP"
      },
      "source": [
        "#funzione oversampling \"alternativo\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnz9vHQYKQti"
      },
      "source": [
        "#Funzione per fare oversampling solo sul training (nella fase di K-fold split)\n",
        "def oversampling_training(dataset_training, y_status, y_tipology): \n",
        "  data = {\"STATUS\": y_status, \"Tipologia\": y_tipology} \n",
        "  data_class = pd.concat(data, axis = 1) \n",
        "  #show the dataframe \n",
        "  #print(data_class)\n",
        "  tuple_class = list(map(tuple, data_class.to_numpy()))\n",
        "  #print(len(tuple_class))  #45 sample\n",
        "  encode_tuple = { (0,0) : 1, (0,1) : 2, (0,2) : 3, (1,0) : 4, (1,1) : 5, (1,2) : 6}\n",
        "  macro_class = []\n",
        "  for couple in tuple_class: \n",
        "    #print(encode_tuple[couple])\n",
        "    macro_class.append(encode_tuple[couple])\n",
        "\n",
        "  series_macro_class = pd.Series((v for v in macro_class) )\n",
        " \n",
        "  print(\"NUMERO DI TUPLE PER CLASSE MACRO:\")\n",
        "  print(series_macro_class.value_counts())  #<----vedi problema qui: #Expected n_neighbors <= n_samples,  but n_samples = 2, n_neighbors = 3\n",
        "  table_count = series_macro_class.value_counts()\n",
        "  table_count = table_count.to_dict()\n",
        " \n",
        "  n_sintetic = 0\n",
        "  #oversampling alternativo                  #FACCIO OVERSAMPLING SOLO DELLE SOTTOCLASSI DEBOLI                                     \n",
        "  oversample = SMOTE({1: table_count[1]+ n_sintetic ,2: table_count[2]+ n_sintetic , 6: table_count[6]+ n_sintetic,\n",
        "                      3: table_count[6]+n_sintetic , 4: table_count[1]+n_sintetic, 5: table_count[2]+n_sintetic}, k_neighbors=1) \n",
        "    \n",
        " \n",
        "  data_objs_oversampling, macro_class_oversampling = oversample.fit_resample(dataset_training, series_macro_class)\n",
        "\n",
        "  macro= pd.Series((v for v in macro_class_oversampling) )\n",
        "  print(\"NUMERO DI TUPLE PER CLASSE MACRO:\")\n",
        "  print(macro.value_counts())\n",
        "  #DALL'OVERSAMPLING DELLA MACRO_CLASS DEVO CODIFICARE LE NUOVE ETICHETTE\n",
        "  micro_class = list()\n",
        "  for encoded in macro_class_oversampling: \n",
        "    #print(encoded)\n",
        "    for key, value in encode_tuple.items():  #  in dictionary.iteritems():  (for Python 2.x)\n",
        "      if encoded == value:\n",
        "          #print(key)\n",
        "          micro_class.append(key)\n",
        "  #print(micro_class)\n",
        "  new_df = pd.DataFrame(micro_class, columns=['STATUS', 'Tipologia'])\n",
        "  #print(new_df)\n",
        "  y=new_df['STATUS']\n",
        "  #print(y)\n",
        "  #print(\"Nuova lunghezza del vettore target y (dopo l'oversampling): \", len(y))\n",
        "  return data_objs_oversampling, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NUH1BeVMye6"
      },
      "source": [
        "import itertools\n",
        "#Evaluation of Model - Confusion Matrix Plot\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Reds):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\")\n",
        "                # color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWuqsymBJBpA"
      },
      "source": [
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "i=0\n",
        "#AGGIUNGERE PRECISION, RECALL, E MATRICI DI CONFUSIONE\n",
        "dict_model = { \"logit\": {\"accuracy\": [], \"precision\": [], \"recall\": []},\n",
        "                 \"svm\": {\"accuracy\": [], \"precision\": [], \"recall\": []},\n",
        "                 \"rf\": {\"accuracy\":  [], \"precision\": [], \"recall\": []}, \n",
        "                 \"nb\": {\"accuracy\":  [], \"precision\": [], \"recall\": []},\n",
        "                 \"knn\": {\"accuracy\":  [], \"precision\": [], \"recall\": []},\n",
        "              }\n",
        "\n",
        "#variance = [95, 90, 80, 70, 60]\n",
        "# Riduzione feature\n",
        "#for var_val in variance:   #se vogliamo provare a mantenere una percentuale diversa di varianza tra i dati\n",
        "var_val = 95  \n",
        "pca = PCA(float(\"0.\"+ str(var_val) ))   #manteniamo un numero di componenti tale che la varianza venga  mantenuta al 95%\n",
        "pca.fit(data_objs)\n",
        "print(pca.explained_variance_)\n",
        "print(\"Varianza mantenuta:  %\", str(var_val))\n",
        "print(\"Componenti Pricipali: \", len(pca.components_))\n",
        "\n",
        "df = pd.DataFrame({'var':pca.explained_variance_, 'PC': [str(i+1) for i in range(0, len(pca.explained_variance_))]})\n",
        "sns.barplot(x='PC', y='var', data=df, color=\"b\")\n",
        "\n",
        "for train_idx, test_idx in fold_obj.split(data_objs, y):  \n",
        "      i += 1\n",
        "     \n",
        "      X_train, X_test = data_objs[train_idx, :], data_objs[test_idx, :]\n",
        "      y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "      dataset_training = data_objs[train_idx, :]\n",
        "      y_status = y[train_idx]\n",
        "      y_tipology = col_typology[train_idx]\n",
        "      \n",
        "          \n",
        "      #fare oversampling su X_train e y_train\n",
        "      #oversampling_training(dataset_training, y_status, y_tipology) #Test only function\n",
        "\n",
        "      X_oversampled, y_oversampled = oversampling_training(dataset_training, y_status, y_tipology)\n",
        "\n",
        "      #print(\"X_train oversampled\", len(X_oversampled))\n",
        "      #print(\"y_train oversampled\", len(y_oversampled))\n",
        "\n",
        "      X_train = X_oversampled\n",
        "      y_train = y_oversampled\n",
        "\n",
        "      print(\"TRAINING sample: \", len(X_train))\n",
        "      print(\"TEST sample: \", len(X_test))\n",
        "\n",
        "      # scatter plot of examples by class label\n",
        "      #BUILD Logit\n",
        "      logisticRegr = LogisticRegression(solver = 'lbfgs')  # handle L2 or no penalty\n",
        "      logisticRegr.fit(X_train, y_train)\n",
        "      test_predicted_logit[test_idx]  = logisticRegr.predict(X_test)\n",
        "      print(\"Fold: \" + str(i))\n",
        "      #print(\"Mean Accuracy Logistic Regression:  %\", logisticRegr.score(test_rad,y_test)*100)\n",
        "\n",
        "  \n",
        "      #BUILD SVC\n",
        "      model_svc = SVC(kernel='rbf')\n",
        "      model_svc.fit(X_train, y_train) \n",
        "      test_predicted_svc[test_idx] = model_svc.predict(X_test)\n",
        "      \n",
        "      #print(\"Fold: \" + str(i))\n",
        "      #print(\"\\t Accuracy SVM: %\", accuracy_score(y_test, test_predicted_svc[test_idx])*100)\n",
        "\n",
        "      #BUILD RANDOM FOREST \n",
        "      clf = RandomForestClassifier(max_depth=None, random_state=0)\n",
        "      clf.fit(X_train, y_train) \n",
        "      test_predicted_clf[test_idx] = clf.predict(X_test)\n",
        "      #print(\"\\t Accuracy RF:  %\", accuracy_score(y_test, test_predicted_clf[test_idx])*100) \n",
        "             \n",
        "      #tn, fp, fn, tp = confusion_matrix(y_test, test_predicted_clf[test_idx]).ravel()\n",
        "      #print(\"TP rf \", tp, \"TN rf:\", tn, \"FP rf \", fp, \"FN rf \", fn)  \n",
        "      \n",
        "      from sklearn.naive_bayes import GaussianNB\n",
        "      #BUILD NAIVE BAYESIAN \n",
        "      model_naiveBay = GaussianNB()\n",
        "      model_naiveBay.fit(X_train, y_train)\n",
        "      test_predicted_bayes[test_idx] = model_naiveBay.predict(X_test)\n",
        "      #print(\"\\t Accuracy NB:  %\", accuracy_score(y_test, test_predicted_bayes[test_idx])*100)\n",
        "\n",
        "      #BUILD KNN\n",
        "      neigh = KNeighborsClassifier(n_neighbors=5)\n",
        "      neigh.fit(X_train, y_train)\n",
        "      test_predicted_knn[test_idx] = neigh.predict(X_test)\n",
        "      \n",
        "\n",
        "print(\"End of kfold\") \n",
        "\n",
        "# Compute confusion matrix\n",
        "label = [\"logit\", \"svm\", \"nb\", \"rf\", \"knn\"]  \n",
        "dict_pred = {  \"logit\": {\"y_pred\": []},\n",
        "                 \"svm\": {\"y_pred\": []},\n",
        "                 \"rf\":  {\"y_pred\": []},\n",
        "                 \"nb\":  {\"y_pred\": []},\n",
        "                 \"knn\":  {\"y_pred\": []}, \n",
        "              }\n",
        "  #SVM\n",
        "dict_pred['svm']['y_pred'].append(test_predicted_svc)\n",
        "  #Logit \n",
        "dict_pred['logit']['y_pred'].append(test_predicted_logit)\n",
        "  #Rf\n",
        "dict_pred['rf']['y_pred'].append(test_predicted_clf)\n",
        "  #Nb\n",
        "dict_pred['nb']['y_pred'].append(test_predicted_bayes)\n",
        "  #Nb\n",
        "dict_pred['knn']['y_pred'].append(test_predicted_knn)\n",
        "\n",
        "for name in label: \n",
        "  y_pred = dict_pred[name]['y_pred']\n",
        "  print(y_pred[0])\n",
        "  cnf_matrix = confusion_matrix(y, y_pred[0])\n",
        "  np.set_printoptions(precision=2)\n",
        "  print(cnf_matrix)\n",
        "  plt.figure()\n",
        "  tit = 'Confusion matrix with MinMaxScaler -' + name \n",
        "  plot_confusion_matrix(cnf_matrix, classes=['0','1'],\n",
        "                        title= tit)\n",
        "  #plt.savefig('/content/drive/My Drive/dati/last_test/Plot/' + tit +'.png')\n",
        "\n",
        "\n",
        "print(\"\\n\\nRisultati metriche: \\n\")\n",
        "\n",
        "print(\"Accuracy SVM: \", accuracy_score(y,test_predicted_svc))\n",
        "print(\"Accuracy RF: \", accuracy_score(y,test_predicted_clf))\n",
        "print(\"Accuracy NB: \", accuracy_score(y,test_predicted_bayes))\n",
        "print(\"Accuracy LOGIT: \", accuracy_score(y,test_predicted_logit))\n",
        "print(\"Accuracy KNN: \", accuracy_score(y,test_predicted_knn))\n",
        "\n",
        "print(\"\\nPrecision SVM: \", precision_score(y,test_predicted_svc))\n",
        "print(\"Precision RF: \", precision_score(y,test_predicted_clf))\n",
        "print(\"Precision NB: \", precision_score(y,test_predicted_bayes))\n",
        "print(\"Precision LOGIT: \", precision_score(y,test_predicted_logit))\n",
        "print(\"Precision KNN: \", precision_score(y,test_predicted_knn))\n",
        "\n",
        "print(\"\\nRecall SVM: \", recall_score(y,test_predicted_svc))\n",
        "print(\"Recall RF: \", recall_score(y,test_predicted_clf))\n",
        "print(\"Recall NB: \", recall_score(y,test_predicted_bayes))\n",
        "print(\"Recall LOGIT: \", recall_score(y,test_predicted_logit))\n",
        "print(\"Recall KNN: \", recall_score(y,test_predicted_knn))\n",
        "\n",
        "#plot_confusion_matrix(clf, data_objs, y)  # doctest: +SKIP\n",
        "#plt.show() \n",
        "\n",
        "  #SVM\n",
        "dict_model['svm']['accuracy'].append(accuracy_score(y, test_predicted_svc))\n",
        "  #Logit \n",
        "dict_model['logit']['accuracy'].append(accuracy_score(y, test_predicted_logit))\n",
        "  #Rf\n",
        "dict_model['rf']['accuracy'].append(accuracy_score(y, test_predicted_clf))\n",
        "  #Nb\n",
        "dict_model['nb']['accuracy'].append(accuracy_score(y, test_predicted_bayes))\n",
        "  #KNN\n",
        "dict_model['knn']['accuracy'].append(accuracy_score(y, test_predicted_knn))\n",
        "\n",
        "\n",
        "  #SVM\n",
        "dict_model['svm']['precision'].append(precision_score(y, test_predicted_svc))\n",
        "  #Logit \n",
        "dict_model['logit']['precision'].append(precision_score(y, test_predicted_logit))\n",
        "  #Rf\n",
        "dict_model['rf']['precision'].append(precision_score(y, test_predicted_clf))\n",
        "  #Nb\n",
        "dict_model['nb']['precision'].append(precision_score(y, test_predicted_bayes))\n",
        "  #KNN\n",
        "dict_model['knn']['precision'].append(precision_score(y, test_predicted_knn))\n",
        "\n",
        "\n",
        "\n",
        "  #SVM\n",
        "dict_model['svm']['recall'].append(recall_score(y, test_predicted_svc))\n",
        "  #Logit \n",
        "dict_model['logit']['recall'].append(recall_score(y, test_predicted_logit))\n",
        "  #Rf\n",
        "dict_model['rf']['recall'].append(recall_score(y, test_predicted_clf))\n",
        "  #Nb\n",
        "dict_model['nb']['recall'].append(recall_score(y, test_predicted_bayes))\n",
        "  #KNN\n",
        "dict_model['knn']['precision'].append(recall_score(y, test_predicted_knn))\n",
        "\n",
        "#j=0\n",
        "\n",
        "'''\n",
        "for key in dict_model.keys():   \n",
        "    #print(j)\n",
        "    np.save('/content/drive/My Drive/dati/last_test/StandardScaler/NO_PCA/accuracy_'+ str(key)+ '_NO_PCA' + str(n_fold) + '-fold', list(dict_model[key]['accuracy']))\n",
        "    #np.savetxt('/content/drive/My Drive/dati/last_test/StandardScaler/NO_PCA/accuracy_'+ str(key)+ '_NO_PCA' + str(n_fold) + '-fold.txt', list(dict_model[key]['accuracy'])) \n",
        "    np.save('/content/drive/My Drive/dati/last_test/StandardScaler/NO_PCA/precision_'+ str(key)+ '_NO_PCA' + str(n_fold) + '-fold', list(dict_model[key]['precision']))\n",
        "    np.save('/content/drive/My Drive/dati/last_test/StandardScaler/NO_PCA/recall_'+ str(key)+ '_NO_PCA' + str(n_fold) + '-fold', list(dict_model[key]['recall']))\n",
        "    #j+=1\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}